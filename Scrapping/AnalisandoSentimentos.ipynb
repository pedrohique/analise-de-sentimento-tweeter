{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas que iremos utilizar:\n",
    "from nltk import word_tokenize #ja é um modulo pronto para twitter\n",
    "import nltk #biblioteca com ferramentas para trabalhar com textos\n",
    "import re #modulo para fazer a limpeza dos dados\n",
    "import pandas as pd #biblioteca pandas para trabalharmos melhor com a base de dados\n",
    "from sklearn.feature_extraction.text import CountVectorizer #modulo de vetorização dos tweets\n",
    "from sklearn.naive_bayes import MultinomialNB #modelo de machine learn que iremos utilizar\n",
    "from sklearn import svm #outro modelo de machine learn para utilizarmos como base\n",
    "from sklearn import metrics #modulo de metrificação(?) para compararmos nosso resultado\n",
    "from sklearn.model_selection import cross_val_predict #modulo de validação para cruzar o modelo com o padrão fornecido\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo a base de dados:\n",
    "df = pd.read_csv('Dados/Tweets_Mg.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geo Coordinates.latitude</th>\n",
       "      <th>Geo Coordinates.longitude</th>\n",
       "      <th>User Location</th>\n",
       "      <th>Username</th>\n",
       "      <th>User Screen Name</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jan 08 01:22:05 +0000 2017</td>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Leonardo C Schneider</td>\n",
       "      <td>LeoCSchneider</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jan 08 01:49:01 +0000 2017</td>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jan 08 01:01:46 +0000 2017</td>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Wed Jan 04 21:43:51 +0000 2017</td>\n",
       "      <td>��� https://t.co/BnDsO34qK0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ana estudando</td>\n",
       "      <td>estudandoconcur</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon Jan 09 15:08:21 +0000 2017</td>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Milly777</td>\n",
       "      <td>0</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      Created At  \\\n",
       "0           0  Sun Jan 08 01:22:05 +0000 2017   \n",
       "1           1  Sun Jan 08 01:49:01 +0000 2017   \n",
       "2           2  Sun Jan 08 01:01:46 +0000 2017   \n",
       "3           3  Wed Jan 04 21:43:51 +0000 2017   \n",
       "4           4  Mon Jan 09 15:08:21 +0000 2017   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ���⛪ @ Catedral de Santo Antônio - Governador ...   \n",
       "1  � @ Governador Valadares, Minas Gerais https:/...   \n",
       "2  �� @ Governador Valadares, Minas Gerais https:...   \n",
       "3                        ��� https://t.co/BnDsO34qK0   \n",
       "4  ��� PSOL vai questionar aumento de vereadores ...   \n",
       "\n",
       "   Geo Coordinates.latitude  Geo Coordinates.longitude User Location  \\\n",
       "0                       NaN                        NaN        Brasil   \n",
       "1                  -41.9333                     -18.85           NaN   \n",
       "2                  -41.9333                     -18.85           NaN   \n",
       "3                       NaN                        NaN           NaN   \n",
       "4                       NaN                        NaN           NaN   \n",
       "\n",
       "               Username User Screen Name  Retweet Count Classificacao  ...  \\\n",
       "0  Leonardo C Schneider    LeoCSchneider              0        Neutro  ...   \n",
       "1               Wândell         klefnews              0        Neutro  ...   \n",
       "2               Wândell         klefnews              0        Neutro  ...   \n",
       "3         Ana estudando  estudandoconcur              0        Neutro  ...   \n",
       "4                 Emily         Milly777              0      Negativo  ...   \n",
       "\n",
       "  Unnamed: 15  Unnamed: 16  Unnamed: 17  Unnamed: 18  Unnamed: 19  \\\n",
       "0         NaN          NaN          NaN          NaN          NaN   \n",
       "1         NaN          NaN          NaN          NaN          NaN   \n",
       "2         NaN          NaN          NaN          NaN          NaN   \n",
       "3         NaN          NaN          NaN          NaN          NaN   \n",
       "4         NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 20  Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  \n",
       "0          NaN          NaN          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando os dados:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positivo    3300\n",
       "Neutro      2453\n",
       "Negativo    2446\n",
       "Name: Classificacao, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a base de dados ja possui uma classificação do comentario para utilizarmos como base,\n",
    "# principalmente na hora de avaliar o nosso modelo\n",
    "df.Classificacao.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14f41877d48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizando esses dados graficamente:\n",
    "df.Classificacao.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                   8199\n",
       "Created At                   8199\n",
       "Text                         8199\n",
       "Geo Coordinates.latitude      104\n",
       "Geo Coordinates.longitude     104\n",
       "User Location                5489\n",
       "Username                     8199\n",
       "User Screen Name             8199\n",
       "Retweet Count                8199\n",
       "Classificacao                8199\n",
       "Observação                      1\n",
       "Unnamed: 10                     0\n",
       "Unnamed: 11                     0\n",
       "Unnamed: 12                     0\n",
       "Unnamed: 13                     0\n",
       "Unnamed: 14                     0\n",
       "Unnamed: 15                     0\n",
       "Unnamed: 16                     0\n",
       "Unnamed: 17                     0\n",
       "Unnamed: 18                     0\n",
       "Unnamed: 19                     0\n",
       "Unnamed: 20                     0\n",
       "Unnamed: 21                     0\n",
       "Unnamed: 22                     0\n",
       "Unnamed: 23                     0\n",
       "Unnamed: 24                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contando o tamanho da base de dados\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retirando tweets repetidos e retweetados\n",
    "df.drop_duplicates(['Text'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5765"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recontagem\n",
    "df.Text.count() #podemos ver que nosso df diminuiu bastante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separando tweets e suas classificações\n",
    "tweets = df['Text']\n",
    "classes = df['Classificacao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download dos dicionarios lexicos de palavras\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções que iremos ou não utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover Stopwords da nossa base. Exemplos A, E, I, O, artigos, etc\n",
    "def RemoveStopWords(instancia):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords] #comando responsavel por tirar as stopwords das frases\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o stemming em nossa base. Essa função reduz as palavras ao seu sulfixo ex: frequentemente, frequentou, frequentava = frequente\n",
    "def Stemming(instancia):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    palavras = []\n",
    "    for w in instancia.split():\n",
    "        palavras.append(stemmer.stem(w))\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove links, pontos, virgulas,ponto e virgulas dos tweets\n",
    "def Limpeza_dados(instancia):\n",
    "    instancia = re.sub(r\"http\\S+\", \"\", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')\n",
    "    return (instancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mais um stemming de palavras, porem não funciona em portugues esta aqui apenas para fins de estudo.\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def Lemmatization(instancia):\n",
    "  palavras = []\n",
    "  for w in instancia.split():\n",
    "    palavras.append(wordnet_lemmatizer.lemmatize(w))\n",
    "  return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entenda como cada função funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eu gosto partido, tambem votaria novamente nesse governante!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RemoveStopWords('Eu não gosto do partido, e tambem não votaria novamente nesse governante!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eu não gost do partido, e tamb não vot nov ness governante!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stemming('Eu não gosto do partido, e tambem não votaria novamente nesse governante!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�� @ governador valadares, minas gerais '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Limpeza_dados('�� @ Governador Valadares, Minas Gerais https:uol.com.br')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Os carros são bonito'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lemmatization('Os carros são bonitos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podemos fazer uma função com as 4 funções anteriores funcionando juntas\n",
    "def Preprocessing(instancia):\n",
    "    instancia = re.sub(r\"http\\S+\", \"\", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','').replace('\"','')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ���⛪ @ Catedral de Santo Antônio - Governador ...\n",
       "1    � @ Governador Valadares, Minas Gerais https:/...\n",
       "2    �� @ Governador Valadares, Minas Gerais https:...\n",
       "3                          ��� https://t.co/BnDsO34qK0\n",
       "4    ��� PSOL vai questionar aumento de vereadores ...\n",
       "5    \" bom é bandido morto\"\\nDeputado Cabo Júlio é ...\n",
       "6    \"..E 25% dos mineiros dizem não torcer para ti...\n",
       "7    \"A gigantesca barba do mal\" em destaque no cad...\n",
       "8    \"BB e governo de Minas travam disputa sobre de...\n",
       "9    \"com vcs bh fica pequena!\" Belo Horizonte (pro...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:10] #Antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [Preprocessing(i) for i in tweets] #aplica a função em todos os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[:10] #Depois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tolkenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tolkenização consiste em separar as paralavras em letras para tentar filtrar o uso não padrão da linguagem do twitter.\n",
    "#Exemplo: ‘@Pedro A Fiap é Legal :) :)’\n",
    "#Vai ficar: ['@','Pedro',','','A','fiap','é','legal',':',')',':',')']\n",
    "from nltk.tokenize import word_tokenize #importa a biblioteca de tokenização do NLTK\n",
    "frase = '@Pedro, a fiap é legal :) :)'\n",
    "word_tokenize(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#porem na biblioteca nltk existe um tokenizador proprio para tweeter.\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devemos intancia-lo como objeto\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer.tokenize(frase) #Repare que os emoticons ficam juntos facilitando a analise do sentimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o modelo para classificar os tweets em positivo, negativo ou neutro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o objeto que faz a vetorização dos dados de texto e compara com os seguintes dados:\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize) #ja estamos passando o tokenizador que queremos que ele utilize\n",
    "# Ou seja esse programa faz a leitura da frase e a transforma palavas em colunas comparando com as frases da segunda linha\n",
    "# e assim sucessivamente.\n",
    "#exemplo:1º frase-  @Pedro A fiap é legal\n",
    "                   #   1   1  1   1   1  \n",
    "        #2º frase - @joão eu estudo na fiap    #compara a primeira frase com a segunda\n",
    "        #              0   0    0    0  2\n",
    "        #3º frase - @Maria eu não estudo na fiap #compara a segunda fase com a terceira\n",
    "        #              0    1   0    1    1   3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assim podemos criar um frequencia de palavras chave\n",
    "freq_tweets = vectorizer.fit_transform(tweets)\n",
    "type(freq_tweets) #repare que ele forma uma matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos checar o tamanho dessa matrix\n",
    "freq_tweets.shape #(cada palavra da nossa base virou uma coluna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino do modelo Machine Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agora que ja vetorizamos os dados vamos treinar o modelo de machine learn que iremos utilizar\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_tweets, classes) #nesta função iremos passar as matriz e as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tweets.A #vamos verificar como esta nossa matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos fazer um teste com nosso modelo \n",
    "# Defina instâncias de teste dentro de uma lista:\n",
    "testes = ['Esse governo está no início, vamos ver o que vai dar',\n",
    "          'Estou muito feliz com o governo de Minas esse ano',\n",
    "          'O estado de Minas Gerais decretou calamidade financeira!!!',\n",
    "          'A segurança desse país está deixando a desejar',\n",
    "          'O governador de Minas é mais uma vez do PT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os dados de teste em vetores de palavras:\n",
    "freq_testes = vectorizer.transform(testes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo a classificação com o modelo treinado:\n",
    "for t, c in zip (testes,modelo.predict(freq_testes)):\n",
    "    # t representa o tweet e c a classificação de cada tweet.\n",
    "    print (t +\", \"+ c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O algoritimo trabalha determinando as probabilidades de cada classe:\n",
    "print (modelo.classes_)\n",
    "modelo.predict_proba(freq_testes).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags de Negações\n",
    "Acrescenta uma tag _NEG encontrada após um ‘não’.\n",
    "Objetivo é dar mais peso para o modelo identificar uma inversão de sentimento da frase.\n",
    "Exemplos:\n",
    "Eu gosto de cachorros, positivo.\n",
    "Eu não gosto de cachorros, negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar as tags de negação:\n",
    "def marque_negacao(texto):\n",
    "    negacoes = ['não','not']\n",
    "    negacao_detectada = False\n",
    "    resultado = []\n",
    "    palavras = texto.split()\n",
    "    for p in palavras:\n",
    "        p = p.lower()\n",
    "        if negacao_detectada == True:\n",
    "            p = p + '_NEG'\n",
    "        if p in negacoes:\n",
    "            negacao_detectada = True\n",
    "        resultado.append(p)\n",
    "    return (\" \".join(resultado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exemplo com negação\n",
    "marque_negacao('Eu nao gosto do partido e tambem não votaria novamente nesse governante!') #O resultado ajudará o moledo de machine learn a ser mais preciso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemplo sem negação\n",
    "marque_negacao('Eu gosto do partido, e tambem votaria novamente nesse governante!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando modelos com Pipelines\n",
    "Pipelines são interessantes para reduzir código e automatizar fluxos.\n",
    "Pipeline são objetos que encadeiam funções para nós facilitando sua vida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizando os dados e passando o classificador:\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline_simples = Pipeline([\n",
    "  ('counts', CountVectorizer()),#vetoriza os dados em matrix\n",
    "  ('classifier', MultinomialNB())#classifica a matrix\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_simples.fit(tweets,classes)#treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline com negação inclusa\n",
    "pipeline_negacoes = Pipeline([\n",
    "  ('counts', CountVectorizer(tokenizer=lambda text: marque_negacao(text))),\n",
    "  ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_negacoes.fit(tweets, classes) #treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline simples com kernel svs para comparação do modelo\n",
    "pipeline_svm_simples = Pipeline([\n",
    "  ('counts', CountVectorizer()),\n",
    "  ('classifier', svm.SVC(kernel='linear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline com tag de negação e kernel svs para comparação do modelo\n",
    "pipeline_svm_negacoes = Pipeline([\n",
    "  ('counts', CountVectorizer(tokenizer=lambda text: marque_negacao(text))),\n",
    "  ('classifier', svm.SVC(kernel='linear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo o cross validation do modelo vai separar o modelo em 10 partes distintas e testar: pipeline simples, tweet(dados), classes, qtd de validaçãos\n",
    "resultados = cross_val_predict(pipeline_simples, tweets, classes, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para determinar a acuracia do modelo: cruza os resultados obtidos com os resultados fornecidos originalmente na tabela\n",
    "metrics.accuracy_score(classes,resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medidas de validação do modelo: cruza os resultados obtidos com os resultados fornecidos originalmente na tabela\n",
    "sentimento=['Positivo','Negativo','Neutro']\n",
    "print (metrics.classification_report(classes,resultados,sentimento))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão: cruza os resultados obtidos com os resultados fornecidos originalmente na tabela\n",
    "print (pd.crosstab(classes, resultados, rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para automatizar todo o processo acima:\n",
    "def Metricas(modelo, tweets, classes): #Modelo que iremos analizar, tweets, classes\n",
    "  resultados = cross_val_predict(modelo, tweets, classes, cv=10)\n",
    "  return 'Acurácia do modelo: {}'.format(metrics.accuracy_score(classes,resultados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metricas naive bayes pipeline simples\n",
    "Metricas(pipeline_simples,tweets,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metricas naive bayes pipeline simples com tag de negacoes:\n",
    "Metricas(pipeline_negacoes,tweets,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM linear simples:\n",
    "Metricas(pipeline_svm_simples,tweets,classes) #podemos notar que a acuracia desse modelo é melhor porem ele é muito mais demorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM linear com tag de negacoes:\n",
    "Metricas(pipeline_svm_negacoes,tweets,classes) #podemos notar que a acuracia desse modelo é melhor porem ele é muito mais demorado pois almenta o numero de processos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
